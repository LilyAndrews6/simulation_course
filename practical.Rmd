---
title: "Which t-test should I use? A simulation study comparing the Student and Welch t-test"
author: "Group 2 - Lily, Daniel, Anna and Em, "
date: "2023-06-22"
output: github_document
---

## Study Design 

### Aim
To compare the performance of two different specifications of the two-sample t test when variances are unequal and group sizes are both equal and unequal 

### Data Generation Mechanisms 

We decided on 9 different DGMS, varying both the sample sizes and the degree to which the variance differed. They were as follows: 

```{r}
nsim <- 1000
dgm <- 1:9
n1 <- c(rep(100,9))
n2 <- c(100,200,1000,100,200,1000,100,200,1000)
mu1 <- c(rep(5,9))
mu2 <- c(rep(5,9))
sd1 <- c(rep(1,9))
sd2 <- c(1,3,0.5,1,3,0.5,1,3,0.5)
df <- data.frame(n1,n2,mu1,mu2,sd1,sd2)
df
```

### Estimand(s) 

The estimand was the difference in means. 

### Methods 
We compared two different t-tests, one of which was just a standard Student's t-test (which assumes equal variances), and one which allows for different variances in the two groups (Welch's t-test). We therefore had two methods: 

1) standard t-test (t_equal)
2) Welch t-test (t_unequal)

### Performance 

We aimed to look at bias and coverage together with monte carlo standard errors. 
We also conducted a number of sense checks: 
- plotting the estimated mean differences 
- comparing the model-based and empirical standard errors 

We ran 1000 simulations. 

## Simulation set-up 

```{r message=FALSE, warning=FALSE}
library(foreach)
library(dplyr)
library(tidyr)
library(ggplot2)
library(gt)

set.seed(1234)

sim <- function(n1,n2,mu1,mu2,sd1,sd2,dgm){

df1 <- data.frame(id = 1:n1, group = 1, value = rnorm(n1,mu1,sd1))
df2 <- data.frame(id = 1:n2, group = 2, value = rnorm(n2,mu2,sd2))

ttest <- t.test(y = df1$value, x = df2$value, paired = FALSE, var.equal = FALSE)
means <- ttest$estimate
diff_means <- means[1] - means[2]
CI_lower <- ttest$conf.int[1]
CI_upper <- ttest$conf.int[2]
SE <- ttest$stderr

ttest1 <- t.test(y = df1$value, x = df2$value, var.equal  = TRUE, paired = FALSE)
means1 <- ttest1$estimate
diff_means1 <- means1[1] - means1[2]
CI_lower1 <- ttest1$conf.int[1]
CI_upper1 <- ttest1$conf.int[2]
SE1 <- ttest1$stderr

output <- data.frame(diff_means,CI_lower,CI_upper,SE,diff_means1,CI_lower1,CI_upper1,SE1,dgm)
colnames(output) <- c("t_unequal","lower_unequal","upper_unequal","SE_unequal","t_equal","lower_equal", "upper_equal","SE_equal","DGM")
output
}
```

Running the simulation:
```{r}
simulation <- foreach(j = dgm) %do% {
  foreach(i=1:nsim, .combine = "rbind") %do% {
  sim(n1[j],n2[j],mu1[j],mu2[j],sd1[j],sd2[j],j)
}
}
test <- data.frame(do.call(rbind.data.frame, simulation))
```

## Results 
### Sense checks 
Check the mean for each DGM in a histogram 
```{r message=FALSE, warning=FALSE}
library(cowplot)
check_mean <- function(dgm){
  plot1 <- ggplot(subset(test, DGM == dgm), aes(x =t_unequal)) +
    geom_histogram(binwidth = 0.05)
  plot2 <- ggplot(subset(test, DGM == dgm), aes(x =t_equal)) +
    geom_histogram(binwidth = 0.05)
  hist_mean <- plot_grid(plot1, plot2)
  hist_mean
}
check_mean(1)
check_mean(2)
check_mean(3)
check_mean(4)
check_mean(5)
check_mean(6)
check_mean(7)
check_mean(8)
check_mean(9)

```
Check mean/se:
```{r}
check_mean_se <- function(dgm){
  plot1 <- ggplot(subset(test, DGM == dgm), aes(x =t_unequal, y=SE_unequal)) +
    geom_point()
  plot2 <- ggplot(subset(test, DGM == dgm), aes(x =t_equal, y=SE_equal)) +
    geom_point()
  hist_mean_se <- plot_grid(plot1, plot2)
  hist_mean_se
}
check_mean_se(1)
check_mean_se(2)
check_mean_se(3)
check_mean_se(4)
check_mean_se(5)
check_mean_se(6)
check_mean_se(7)
check_mean_se(8)
check_mean_se(9)
```

## Results: bias 

bias of t_equal:
```{r}
test %>%
  group_by(DGM) %>%
  summarise(
    bias = mean(t_equal) - (0),
    empSE = sqrt(var(t_equal)),
    modSE = mean(SE_equal)
  ) %>% gt()
```

bias of t_unequal:
```{r}
test %>%
  group_by(DGM) %>%
  summarise(
    bias = mean(t_unequal) - (0),
    empSE = sqrt(var(t_unequal)),
    modSE = mean(SE_unequal) 
  ) %>% gt() 
```

## Full outputs from rsimsum 

results for t_equal
```{r message=FALSE, warning=FALSE}
library(rsimsum)
s <- simsum(data = test, estvarname = "t_equal",
    se = "SE_equal", true = 0, by = "DGM") 
results_equal <- summary(s)
results_equal
```

results for t_uneqal
```{r}
s <- simsum(data = test, estvarname = "t_unequal",
    se = "SE_unequal", true = 0, by = "DGM") 
results_unequal <- summary(s)
results_unequal
```

comparison of bias
```{r, echo = FALSE, message = FALSE, warning = FALSE}
sum_results_equal <- results_equal[[1]] %>% 
  filter(stat == "bias") %>% 
  select(est, mcse, DGM)

sum_results_unequal <- results_unequal[[1]] %>% 
  filter(stat == "bias") %>% 
  select(est, mcse, DGM)

bias_results <- inner_join(sum_results_equal, sum_results_unequal, 
                           by = join_by(DGM), 
                           suffix = c(".equal", ".unequal")) %>% 
  select(DGM, everything())

gt(bias_results)
  
```
comparison of coverage
```{r, echo = FALSE}

sum_results_equal <- results_equal[[1]] %>% 
  filter(stat == "cover") %>% 
  select(est, mcse, DGM)

sum_results_unequal <- results_unequal[[1]] %>% 
  filter(stat == "cover") %>% 
  select(est, mcse, DGM)

cover_results <- inner_join(sum_results_equal, sum_results_unequal, 
                           by = join_by(DGM), 
                           suffix = c(".equal", ".unequal")) %>% 
  select(DGM, everything())

gt(cover_results)
  
```

comparison of power
```{r, echo = FALSE}

sum_results_equal <- results_equal[[1]] %>% 
  filter(stat == "power") %>% 
  select(est, mcse, DGM)

sum_results_unequal <- results_unequal[[1]] %>% 
  filter(stat == "power") %>% 
  select(est, mcse, DGM)

power_results <- inner_join(sum_results_equal, sum_results_unequal, 
                           by = join_by(DGM), 
                           suffix = c(".equal", ".unequal")) %>% 
  select(DGM, everything())

gt(power_results)
  
```
# visualisations 


visualisations 

results for t_equal
```{r message=FALSE, warning=FALSE}
library(rsimsum)
s <- simsum(data = test, estvarname = "t_equal",
    se = "SE_equal", true = 0, by = "DGM", x = TRUE) 
results_equal <- summary(s)

autoplot(summary(s), type = "lolly", stats = "bias")

```

if we'd use a long format, we'd easily get very nice comparison plots with hardly any code! 
it's described here but there was no time: https://cran.r-project.org/web/packages/rsimsum/vignettes/C-plotting.html 

## Interpretation 
- Surprisingly, the mean and SE was exactly the same for the student and Welsh t-test when the sample size was the same (although p-values were different) 
- The empirical SE was quite different from the model-based SE when the SD was large 
- other thoughts??? 
